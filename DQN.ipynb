{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network w/ tensorflow\n",
    "\n",
    "Revised: YoungJun Kim \n",
    "(02-06-2018)\n",
    "\n",
    "Deep Q network on GridWorld Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import\n",
    "- open AI gym\n",
    "- TensorFlow >= 1.2.0\n",
    "- gridworld.py (pre-defined game environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "import numpy as np\n",
    "import os, sys, random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the Game environment: gridworld.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load environments of gridworld\n",
    "- compose of pixels: Total 84x84x3\n",
    "- move the blue square to green squares while avioiding red squares\n",
    "- reward: +1 for reaching green square, -1 for reaching red square\n",
    "- The position of the three blocks is randomized every episode.\n",
    "\n",
    "\n",
    "- env = gameEnv(partial=False, size=5): 5x5 grid, fully observable environment defined\n",
    "- state = env.reset(): returns new randomized environment for next episode\n",
    "- [next_state, reward, done] = env.step(action): returns next_state, reward, and done value for given action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC+hJREFUeJzt3V2MXdV9hvHnrYGQQBtwTBHF0PEFIrIiYVKLQomqFHDk\n0Ij0CoFEFVVI3KStqSJFob1AueOiipKLKhIKSVFDSakDDbIiUpIQVZUqB/PRFPwRE2KCXYiHtCkp\nldI6+ffibIvB8scez5kzs1jPTxrN2esc6+zl8Tt7z/ae9aaqkNSfX1npHZC0Mgy/1CnDL3XK8Eud\nMvxSpwy/1CnDL3VqSeFPsjXJviQvJPnUtHZK0vLL6d7kk2QN8H1gC3AQeBK4tap2T2/3JC2XM5bw\nZ68CXqiqFwGSfAX4KHDC8K9bt67m5uaW8JaSTubAgQO89tprGfPapYT/YuDlBdsHgd8+2R+Ym5tj\n165dS3hLSSezefPm0a9d9gt+Se5IsivJrvn5+eV+O0kjLSX8h4BLFmyvH8beoqrurarNVbX5ggsu\nWMLbSZqmpYT/SeCyJBuSnAXcAjw6nd2StNxO+2f+qjqS5I+BbwBrgC9W1fNT2zNJy2opF/yoqq8D\nX5/SvkiaIe/wkzpl+KVOGX6pU4Zf6pThlzpl+KVOGX6pU4Zf6pThlzpl+KVOGX6pU4Zf6pThlzpl\n+KVOGX6pU4Zf6pThlzp1yvAn+WKSw0meWzC2NsnjSfYPn89f3t2UNG1jjvx/DWw9ZuxTwLeq6jLg\nW8O2pIacMvxV9U/Afxwz/FHg/uHx/cAfTHm/JC2z0/2Z/8KqemV4/Cpw4ZT2R9KMLPmCX02aPk/Y\n9mljj7Q6nW74f5zkIoDh8+ETvdDGHml1Ot3wPwp8bHj8MeBr09kdSbNyytKOJA8CHwTWJTkI3A3c\nAzyU5HbgJeDm5dzJacio0uJle/eVfHOtgMlPw6vbKcNfVbee4Knrp7wvkmbIO/ykThl+qVOGX+qU\n4Zc6ZfilThl+qVOGX+qU4Zc6ZfilThl+qVOGX+qU4Zc6ZfilThl+qVOGX+qU4Zc6ZfilTo1p7Lkk\nyRNJdid5Psm2YdzWHqlhY478R4BPVNVG4Grg40k2YmuP1LQxjT2vVNXTw+OfAXuAi7G1R2raon7m\nTzIHXAnsZGRrj6Ud0uo0OvxJzgW+CtxZVa8vfO5krT2Wdkir06jwJzmTSfAfqKqHh+HRrT2SVp8x\nV/sD3AfsqarPLHjK1h6pYacs7QCuBf4Q+Lckzw5jf06DrT2S3jSmseefOXHflK09UqO8w0/qlOGX\nOmX4pU6NueD3NmFN9opYyaZqv+Qn5ZFf6pThlzpl+KVOGX6pU4Zf6pThlzpl+KVOGX6pU4Zf6pTh\nlzpl+KVOGX6pU4Zf6tSYNfzOTvLdJP86NPZ8ehi3sUdq2Jgj/8+B66rqCmATsDXJ1djYIzVtTGNP\nVdV/D5tnDh+FjT1S08au279mWLn3MPB4VdnYIzVuVPir6hdVtQlYD1yV5H3HPG9jj9SYRV3tr6qf\nAk8AW7GxR2ramKv9FyQ5b3j8TmALsBcbe6SmjVnA8yLg/iRrmHyzeKiqdiT5F2zskZo1prHne0xq\nuY8d/wk29kjN8g4/qVOGX+qU4Zc6ZfilThl+qVOGX+qU4Zc6ZfilTnVU0d2nlWzIBshK1mSv9ORX\nOY/8UqcMv9Qpwy91yvBLnTL8UqcMv9Qpwy91yvBLnRod/mH57meS7Bi2beyRGraYI/82YM+CbRt7\npIaNLe1YD/w+8IUFwzb2SA0be+T/LPBJ4JcLxmzskRo2Zt3+jwCHq+qpE73Gxh6pPWN+q+9a4KYk\nNwJnA7+W5MsMjT1V9YqNPVJ7xrT03lVV66tqDrgF+HZV3YaNPVLTlvL//PcAW5LsB24YtiU1YlGL\neVTVd4DvDI9t7JEa5h1+UqcMv9Qpwy91yvBLnTL8UqcMv9Qpwy91yvBLnTL8UqcMv9Qpwy91yvBL\nnTL8UqcMv9SpRf1Kr9qTld6BlbSSkz/uonari0d+qVOjjvxJDgA/A34BHKmqzUnWAn8HzAEHgJur\n6j+XZzclTdtijvy/V1WbqmrzsG1ph9SwpZz2W9ohNWxs+Av4ZpKnktwxjI0q7ZC0Oo292v+BqjqU\n5NeBx5PsXfhkVVWS417fHL5Z3AFw6aWXLmlnJU3PqCN/VR0aPh8GHgGuYijtADhZaYeNPdLqNKau\n65wkv3r0MfAh4Dks7ZCaNua0/0LgkSRHX/+3VfVYkieBh5LcDrwE3Lx8uylp2k4Z/qp6EbjiOOOW\ndkgN8w4/qVOGX+qU4Zc6ZfilThl+qVOGX+qU4Zc6ZfilThl+qVOGX+qU4Zc6ZfilThl+qVOGX+qU\n4Zc6ZfilThl+qVOjwp/kvCTbk+xNsifJNUnWJnk8yf7h8/nLvbOSpmfskf9zwGNV9V4mS3rtwcYe\nqWljVu99N/C7wH0AVfW/VfVTbOyRmjZm9d4NwDzwpSRXAE8B27Cxpw0rXBVdK1iTffwaGR015rT/\nDOD9wOer6krgDY45xa+q4gT/zJLckWRXkl3z8/NL3V9JUzIm/AeBg1W1c9jezuSbgY09UsNOGf6q\nehV4Ocnlw9D1wG5s7JGaNrao80+AB5KcBbwI/BGTbxw29kiNGhX+qnoW2Hycp2zskRrlHX5Spwy/\n1CnDL3XK8EudMvxSpwy/1CnDL3XK8EudMvxSpwy/1CnDL3XK8EudMvxSpwy/1CnDL3XK8EudMvxS\np8as2395kmcXfLye5E4be6S2jVnAc19VbaqqTcBvAf8DPIKNPVLTFnvafz3wg6p6CRt7pKYtNvy3\nAA8Oj23skRo2OvzDst03AX9/7HM29kjtWcyR/8PA01X142Hbxh6pYYsJ/628ecoPNvZITRsV/iTn\nAFuAhxcM3wNsSbIfuGHYltSIsY09bwDvOWbsJzTU2DO5LKGZW8G/dr/iJ+cdflKnDL/UKcMvdcrw\nS50y/FKnDL/UKcMvdcrwS50y/FKnDL/UKcMvdcrwS50y/FKnDL/UKcMvdcrwS50y/FKnxi7j9WdJ\nnk/yXJIHk5xtY4/UtjF1XRcDfwpsrqr3AWuYrN9vY4/UsLGn/WcA70xyBvAu4N+xsUdq2piuvkPA\nXwI/Al4B/quq/hEbe6SmjTntP5/JUX4D8BvAOUluW/gaG3uk9ow57b8B+GFVzVfV/zFZu/93sLFH\natqY8P8IuDrJu5KEyVr9e7CxR2raKUs7qmpnku3A08AR4BngXuBc4KEktwMvATcv545Kmq6xjT13\nA3cfM/xzGmrskfRW3uEndcrwS50y/FKnDL/UqcyyujrJPPAG8NrM3nT5rcP5rGZvp/mMmctvVtWo\nG2pmGn6AJLuqavNM33QZOZ/V7e00n2nPxdN+qVOGX+rUSoT/3hV4z+XkfFa3t9N8pjqXmf/ML2l1\n8LRf6tRMw59ka5J9SV5I0tSyX0kuSfJEkt3DeobbhvGm1zJMsibJM0l2DNvNzifJeUm2J9mbZE+S\naxqfz7KunTmz8CdZA/wV8GFgI3Brko2zev8pOAJ8oqo2AlcDHx/2v/W1DLcx+RXto1qez+eAx6rq\nvcAVTObV5HxmsnZmVc3kA7gG+MaC7buAu2b1/sswn68BW4B9wEXD2EXAvpXet0XMYf3wD+g6YMcw\n1uR8gHcDP2S4jrVgvNX5XAy8DKxl8tu3O4APTXM+szztPzqZow4OY81JMgdcCeyk7bUMPwt8Evjl\ngrFW57MBmAe+NPwY84Uk59DofGoGa2d6wW+RkpwLfBW4s6peX/hcTb4dN/HfJ0k+AhyuqqdO9JqW\n5sPk6Ph+4PNVdSWT28jfckrc0nyWunbmGLMM/yHgkgXb64exZiQ5k0nwH6iqh4fhUWsZrkLXAjcl\nOQB8BbguyZdpdz4HgYNVtXPY3s7km0Gr81nS2pljzDL8TwKXJdmQ5CwmFy8eneH7L8mwfuF9wJ6q\n+syCp5pcy7Cq7qqq9VU1x+Rr8e2quo125/Mq8HKSy4eh64HdNDofZrF25owvYtwIfB/4AfAXK31R\nZZH7/gEmp1jfA54dPm4E3sPkotl+4JvA2pXe19OY2wd584Jfs/MBNgG7hq/RPwDnNz6fTwN7geeA\nvwHeMc35eIef1Ckv+EmdMvxSpwy/1CnDL3XK8EudMvxSpwy/1CnDL3Xq/wEn+Bg4gyv6WQAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d9a0b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "# open environment\n",
    "env = gameEnv(partial=False,size=5)\n",
    "\n",
    "# random setting current state\n",
    "state = env.reset()\n",
    "\n",
    "# visualize current state\n",
    "env.visualize(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementing the network itself\n",
    "### 1) Multi-layer CNN\n",
    "- Start from State as a RGB-channel Image\n",
    "- Convolution -> Re-LU Activation -> Max Pooling\n",
    "- Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Fully-Connected Layer\n",
    "- Fully connected Layer connecting Feature Vector output from CNN to Q-value\n",
    "- Compute Q-value with simple Weight and Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Separate Target Network\n",
    "- Target Q value for loss computation will be given from separate target network\n",
    "- Target network will be updated slowly compared to primary network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Loss and Optimize\n",
    "- Loss: Mean Squared Error between Target Q value and Predicted Q value\n",
    "- Optimizer: Adam optimizer will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size, Target=False):\n",
    "        \n",
    "        #h_size: size of final convolution layer connected to fc_layers\n",
    "        \n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        with tf.name_scope(\"{}\".format(\"Target\" if Target else \"Primary\")):\n",
    "            with tf.name_scope(\"Feature_Extraction\"):\n",
    "                xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "                #get state as one-hot vector\n",
    "                self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32, name=\"state\")\n",
    "                #reshape image to 84x84 with 3 channel\n",
    "                self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "\n",
    "                ## Value of each k_size, channel, stride is from original code\n",
    "                ## Can be altered for better performance\n",
    "\n",
    "                # channel num from input_state to final conv 4 (from 3 to h_size)\n",
    "                channel_list = [3, 16, 32]\n",
    "\n",
    "                # k_size / stride for max pooling\n",
    "                k_size_list=[8, 4]\n",
    "                stride_list = [4, 4]\n",
    "\n",
    "                result = self.imageIn #rename for loop\n",
    "\n",
    "                ## through 4 layers of convolution (valid padding) \n",
    "                # conv1 output: batch_size x 21 x 21 x 32\n",
    "                # conv2 output: bath_size x 6 x 6 x 64\n",
    "                # conv3 output: batch_size x 2 x 2 x 128\n",
    "                # conv4 output: batch_size x 1 x 1 x 512 (final)\n",
    "\n",
    "                for i in range(2):\n",
    "                    j = i+1 # for next time_step\n",
    "\n",
    "                    with tf.name_scope(\"conv_layer_{}\".format(j)):\n",
    "\n",
    "                        W = tf.Variable(name=\"conv_w\", \n",
    "                                        initial_value=xavier_init([k_size_list[i], k_size_list[i], \n",
    "                                                                   channel_list[i], channel_list[j]]), \n",
    "                                        dtype=tf.float32)\n",
    "                        b = tf.Variable(name=\"conv_b\", \n",
    "                                        initial_value=xavier_init([channel_list[j]]), \n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "                        # convolution stage: SAME padding with W \n",
    "                        conv = tf.nn.conv2d(result, \n",
    "                                            W, \n",
    "                                            strides=[1, 1, 1, 1], \n",
    "                                            padding=\"SAME\", \n",
    "                                            name=\"conv\")\n",
    "\n",
    "                        # add bias and activate with Re-LU function\n",
    "                        h_out = tf.nn.relu(tf.nn.bias_add(conv, b), \n",
    "                                           name=\"h_out\")\n",
    "\n",
    "                        # max_pooling with pre-defined values\n",
    "                        h_pool = tf.nn.max_pool(value=h_out, \n",
    "                                                ksize=[1, 2, 2, 1], \n",
    "                                                strides=[1, stride_list[i], stride_list[i], 1], \n",
    "                                                padding=\"SAME\",\n",
    "                                                name=\"pool\")\n",
    "\n",
    "                        result = h_pool # rename variables for loop\n",
    "                        \n",
    "                        tf.summary.histogram(\"conv_w_{}\".format(j), W)\n",
    "                        tf.summary.histogram(\"conv_b_{}\".format(j), b)\n",
    "                        tf.summary.histogram(\"h_pool_{}\".format(j), h_pool)\n",
    "\n",
    "                self.conv4 = result # rename for fc layer computation\n",
    "\n",
    "            with tf.name_scope(\"Dueling_fc_layer\"):\n",
    "            # We take the output from the final convolutional layer \n",
    "            # and compute Q value with FC layer\n",
    "            \n",
    "                self.streamFeature = slim.flatten(self.conv4)\n",
    "                \n",
    "                self.W_1 = tf.Variable(xavier_init([h_size, 256]), name=\"weight_fc\")\n",
    "                self.b_1 = tf.Variable(xavier_init([256]), name=\"bias_fc\")\n",
    "                self.W_2 = tf.Variable(xavier_init([256, 4]), name=\"weight_fc\")\n",
    "                self.b_2 = tf.Variable(xavier_init([4]), name=\"bias_fc\")\n",
    "                \n",
    "                z_1 = tf.add(tf.matmul(self.streamFeature,self.W_1), self.b_1)\n",
    "                a_1 = tf.nn.relu(z_1)\n",
    "                z_2 = tf.add(tf.matmul(a_1, self.W_2), self.b_2)\n",
    "                self.Qout = z_2\n",
    "                \n",
    "                # predicted action out of Q-value\n",
    "                self.predict = tf.argmax(self.Qout,1, name=\"predicted_action\")\n",
    "\n",
    "                # tensorboard summary\n",
    "                tf.summary.histogram(\"FC_W1\", self.W_1)\n",
    "                tf.summary.histogram(\"FC_W2\", self.W_2)\n",
    "                tf.summary.histogram(\"Predicted_Q\", self.Qout)\n",
    "\n",
    "            if not Target:\n",
    "                with tf.name_scope(\"Target_Network\"):\n",
    "                    #Below we obtain the loss by taking the sum of squares difference \n",
    "                    #between the target and prediction Q values.\n",
    "                    self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32, name=\"target_Q\")\n",
    "                    self.actions = tf.placeholder(shape=[None],dtype=tf.int32, name=\"target_action\")\n",
    "                    self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "\n",
    "                    # Q value from target network\n",
    "                    self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "                    tf.summary.histogram(\"Target_Q\", self.Q)\n",
    "\n",
    "                with tf.name_scope(\"Optimizer\"):\n",
    "                    # MSE for calculating Loss between Target Q and Predicted Q\n",
    "                    self.td_error = tf.square(self.targetQ - self.Q)\n",
    "                    self.loss = tf.reduce_mean(self.td_error, name=\"Loss\")\n",
    "\n",
    "                    # Adam optimizer for updating the Model\n",
    "                    self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001, name=\"optimizer\")\n",
    "                    self.updateModel = self.trainer.minimize(self.loss, name=\"update\")\n",
    "                    tf.summary.scalar(\"Loss\", self.loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Experience Replay\n",
    "- Experience will be stored in buffer with limited size\n",
    "- Each Experience is a tuple of 5 elements: [state, action, reward, next_state, done]\n",
    "- Experience is added at every step\n",
    "- If buffer size goes beyond the limit, old experiences will be deleted\n",
    "- Expriences will be randomly sampled as a batch and given model network as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        '''set buffers for experience\n",
    "        Args: \n",
    "            buffer_size - how many experience to hold at given timestep'''\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        '''add experience to experience buffer'''\n",
    "        \n",
    "        # if buffer is full, delete the old experience\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "            \n",
    "        # append [state, action, reward, next_state, done] tuple into experience buffer\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        '''sample experience from buffer according to batch size'''\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a simple function to resize our game frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    '''reshaping states into 1D tensor'''\n",
    "    return np.reshape(states,[21168])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Slow Target Update\n",
    "- Target Network will be updated slowly, for the stability of training\n",
    "- Tau: parameter for updating values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    '''Holds operation node for assigning Target values to Target network\n",
    "    Args:\n",
    "        tfVars - Variables for training(weights, bias...)\n",
    "        Tau - rate for updating (low Tau value for slow updates)\n",
    "    Return:\n",
    "        op_holder - tf.assign() operation. input for updateTarget Function'''\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    \n",
    "    # for latter-half part of trainable variables (= for Target network variables)\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        # assigning tau*new_value+(1-tau)*old_values\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    '''run operation defined in updateTargetGraph function'''\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "gamma = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action (epsilon)\n",
    "endE = 0.1 #Final chance of random action (epsilon)\n",
    "annealing_steps = 100. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 50 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 50 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model checkpoint & train model summary\n",
    "h_size = 1152 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Graphing Main Q network (Primary network) and Target Q network\n",
    "- Build Main Q network and Target Q network Graphs\n",
    "- Define saver and writer\n",
    "- Assign function for Target Network update\n",
    "- Set Experience Buffer\n",
    "- Set Epsilon to anneal through episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Primary/Feature_Extraction/conv_layer_1/pool:0\", shape=(?, 21, 21, 16), dtype=float32)\n",
      "Tensor(\"Primary/Feature_Extraction/conv_layer_2/pool:0\", shape=(?, 6, 6, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#main_graph = tf.Graph()\n",
    "\n",
    "#with main_graph.as_default():\n",
    "tf.reset_default_graph()\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# set network for main model\n",
    "mainQN = Qnetwork(h_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Target/Feature_Extraction/conv_layer_1/pool:0\", shape=(?, 21, 21, 16), dtype=float32)\n",
      "Tensor(\"Target/Feature_Extraction/conv_layer_2/pool:0\", shape=(?, 6, 6, 32), dtype=float32)\n",
      "Graphing Done!\n"
     ]
    }
   ],
   "source": [
    "# set network for target Q-value computation\n",
    "targetQN = Qnetwork(h_size, True)\n",
    "\n",
    "# initialize the varibales\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# saver for model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "writer = tf.summary.FileWriter(logdir=path)\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "# get all the trainable variables\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "# update operation for target graph, ratio for update is defined as tau\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "# set default buffer to hold experiences of episodes\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "reward_List = []\n",
    "\n",
    "print(\"Graphing Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training the Network\n",
    "- Set Load option\n",
    "- Epsilon anneal through episodes\n",
    "- Train Network after pre-training - search the environment before actual training starts\n",
    "- add experience to buffer\n",
    "- Predict action from Main Q and Target Q\n",
    "- Calculate Target Q\n",
    "- Update Main Q parameters\n",
    "- Update Target Q parameters (slowly)\n",
    "- Update buffer\n",
    "- Save and Write Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing all variables...\n",
      "Episode 0/50.Total Steps:50, Mean Rewards:1.4, Epsilon:0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps:150, Mean Rewards:1.4, Epsilon:0.10\n",
      "Total Steps:250, Mean Rewards:1.5, Epsilon:0.10\n",
      "Total Steps:350, Mean Rewards:1.5, Epsilon:0.10\n",
      "Total Steps:450, Mean Rewards:1.2, Epsilon:0.10\n",
      "Saved Model\n",
      "Episode 10/50.Total Steps:550, Mean Rewards:1.2, Epsilon:0.10\n",
      "Total Steps:650, Mean Rewards:1.0, Epsilon:0.10\n",
      "Total Steps:750, Mean Rewards:0.9, Epsilon:0.10\n",
      "Total Steps:850, Mean Rewards:0.8, Epsilon:0.10\n",
      "Total Steps:950, Mean Rewards:0.7, Epsilon:0.10\n",
      "Saved Model\n",
      "Episode 20/50.Total Steps:1050, Mean Rewards:0.8, Epsilon:0.10\n",
      "Total Steps:1150, Mean Rewards:0.9, Epsilon:0.10\n",
      "Total Steps:1250, Mean Rewards:0.7, Epsilon:0.10\n",
      "Total Steps:1350, Mean Rewards:0.7, Epsilon:0.10\n",
      "Total Steps:1450, Mean Rewards:0.7, Epsilon:0.10\n",
      "Saved Model\n",
      "Episode 30/50.Total Steps:1550, Mean Rewards:0.4, Epsilon:0.10\n",
      "Total Steps:1650, Mean Rewards:0.2, Epsilon:0.10\n",
      "Total Steps:1750, Mean Rewards:0.3, Epsilon:0.10\n",
      "Total Steps:1850, Mean Rewards:0.2, Epsilon:0.10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9ae1e9777ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0;31m# Update the network with our target values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     _, curr_sum = sess.run([mainQN.updateModel, summary],                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, \n\u001b[0;32m---> 86\u001b[0;31m                                    mainQN.actions:trainBatch[:,1], targetQN.scalarInput:np.vstack(trainBatch[:,0])})\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;31m# update main QN summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(init)\n",
    "    writer.add_graph(sess.graph)\n",
    "    global_step = 0\n",
    "    \n",
    "    # load model if checkpoint exists\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Initializing all variables...\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # training\n",
    "    for i in range(num_episodes):\n",
    "    \n",
    "        if i % 10 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # re-assign current epsiode buffer\n",
    "        episodeBuffer = experience_buffer()\n",
    "        \n",
    "        # Reset environment and get first new observation\n",
    "        state = env.reset()\n",
    "        state = processState(state)\n",
    "        done = False\n",
    "        reward_All = 0\n",
    "        j = 0\n",
    "        \n",
    "        #The Q-Network\n",
    "        #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "        while j < max_epLength: \n",
    "            j+=1\n",
    "            global_step+=1\n",
    "            \n",
    "            # Epsilon-Greedy Approach\n",
    "            # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or global_step < pre_train_steps:\n",
    "                action = np.random.randint(0,4)\n",
    "            else:\n",
    "                action = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[state]})[0]\n",
    "            \n",
    "            # get next state, reward, and done from environment\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = processState(next_state)\n",
    "            \n",
    "            \n",
    "            #Save the experience to our episode buffer.\n",
    "            episodeBuffer.add(np.reshape(np.array([state, action, reward, next_state, done]),[1,5])) \n",
    "            \n",
    "            # annealing epsilon\n",
    "            if global_step > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                # every update_freq steps, do training from experience buffer\n",
    "                if global_step % (update_freq) == 0:\n",
    "                    \n",
    "                    #Get a random batch of experiences.\n",
    "                    trainBatch = myBuffer.sample(batch_size) \n",
    "                    \n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    \n",
    "                    # predict action of next state (primary)\n",
    "                    predicted_action = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    # predict Q value of next state (target)\n",
    "                    target_Q = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    \n",
    "                    # check if episode ended\n",
    "                    # if end, set future discounted reward to 0\n",
    "                    # as each episode has no end, end_multiplier = 1\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    \n",
    "                    # get Q value of predicted Action\n",
    "                    nextQ = np.max(target_Q)\n",
    "                    \n",
    "                    # calculate Target Q value with Bellman Equation\n",
    "                    # reward + discounted next_state Qvalue\n",
    "                    targetQ = trainBatch[:,2] + (gamma * nextQ * end_multiplier)\n",
    "\n",
    "                    # Update the network with our target values.\n",
    "                    _, curr_sum = sess.run([mainQN.updateModel, summary],\\\n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, \n",
    "                                   mainQN.actions:trainBatch[:,1], targetQN.scalarInput:np.vstack(trainBatch[:,0])})\n",
    "                    \n",
    "                    # update main QN summary\n",
    "                    writer.add_summary(curr_sum, global_step=global_step)\n",
    "                    \n",
    "                    # Update the target network toward the primary network.\n",
    "                    # assign updated value from Main Q with low update rate\n",
    "                    updateTarget(targetOps,sess)\n",
    "            \n",
    "            # save reward\n",
    "            reward_All += reward\n",
    "            # move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            \n",
    "            # if episode is done, break the loop\n",
    "            if done == True:\n",
    "                break\n",
    "        \n",
    "        # update experience buffer of the whole episode to main buffer\n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        \n",
    "        jList.append(j)\n",
    "        reward_List.append(reward_All)\n",
    "        \n",
    "        #Periodically save the model. \n",
    "        if (i+1) % 10 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(reward_List) % 2 == 0:\n",
    "            print(\"Total Steps:{}, Mean Rewards:{}, Epsilon:{:,.2f}\".format(global_step,np.mean(reward_List[-10:]), e))\n",
    "    \n",
    "    # save at the last episode\n",
    "    saver.save(sess, os.path.join(logdir, \"model.ckpt\"), global_step=global_step)\n",
    "    \n",
    "print(\"Percent of succesful episodes: \" + str(sum(reward_List)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reward Result Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rMat = np.resize(np.array(reward_List),[len(reward_List)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Simulation\n",
    "- Reset game environment\n",
    "- Follow greedily according to the action from trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1202af6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set new random game environment\n",
    "state = env.reset()\n",
    "reward_all = 0\n",
    "# show grid layout\n",
    "env.visualize(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## simulation ##\n",
    "# move to next step\n",
    "\n",
    "def next_state(game_env, state, Q, epsilon=0, visulaize=False):\n",
    "    '''move to next state\n",
    "        Args:\n",
    "            game_env: environment for game\n",
    "            state: current state as 84x84x3 image vector\n",
    "            Q: main Q network\n",
    "            epsilon: for epsilon greedy approach (default=0)\n",
    "            visualize: print State after action is applied as an image(default: false)\n",
    "        Returns:\n",
    "            next_state: state after desired action is taken'''\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # load from checkpoint\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "\n",
    "        # process initial state, done and step\n",
    "        state = processState(state)\n",
    "        done = False\n",
    "        j = 0\n",
    "        \n",
    "        # get action from main Q network\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            action = np.random.randint(0,4)\n",
    "        else:\n",
    "            action = sess.run(Q.predict,feed_dict={Q.scalarInput:[state]})[0]\n",
    "        \n",
    "        # move to next state, check reward and if finished\n",
    "        next_state, reward, done = game_env.step(action)\n",
    "\n",
    "        # print next action\n",
    "        possible_action = [\"up\",\"down\",\"left\",\"right\"]\n",
    "        print(\"Action:{}, Reward:{}\".format(possible_action[action], reward))\n",
    "        \n",
    "        # show changed state\n",
    "        if visulaize:\n",
    "            env.visualize(next_state)  \n",
    "        \n",
    "        # if finished, print the result\n",
    "        if done == True:\n",
    "            if reward > 0:\n",
    "                print(\"You Win!\")\n",
    "            else:\n",
    "                print(\"You Lose!\")\n",
    "            \n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./double_dqn/model-0.ckpt\n",
      "Action:down, Reward:1.0\n",
      "Total Reward 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120302320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state, reward = next_state(env, state, targetQN, 0, True)\n",
    "reward_all += reward\n",
    "print(\"Total Reward\", reward_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(game_env, Q, n, epsilon):\n",
    "    '''Args:\n",
    "            game_evnv: environment for game\n",
    "            Q: Q network\n",
    "            n: number of episodes to look\n",
    "            epsilon: for epsilon greedy approach (default=0)\n",
    "        Return:\n",
    "            rewards_list: total cumulative reward of each episode as a list'''\n",
    "    \n",
    "    rewards_list = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    j = 0\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # load from checkpoint\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "\n",
    "        # process initial state, done and step\n",
    "        state = processState(state)\n",
    "        done = False\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            reward_total = 0\n",
    "            j = 0\n",
    "            \n",
    "            while j<50:\n",
    "                \n",
    "                j+=1\n",
    "\n",
    "                # get action from main Q network\n",
    "                if np.random.rand(1) < epsilon:\n",
    "                    action = np.random.randint(0,4)\n",
    "                else:\n",
    "                    action = sess.run(Q.predict,feed_dict={Q.scalarInput:[state]})[0]\n",
    "\n",
    "                # move to next state, check reward and if finished\n",
    "                next_state, reward, done = game_env.step(action)\n",
    "\n",
    "                reward_total += reward\n",
    "                state = processState(next_state)\n",
    "                \n",
    "                if done == True:\n",
    "                    break\n",
    "                    \n",
    "            rewards_list.append(reward_total)\n",
    "            \n",
    "            state = env.reset()\n",
    "            state = processState(state)\n",
    "            \n",
    "    return rewards_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./double_dqn/model-0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24\n"
     ]
    }
   ],
   "source": [
    "RL = performance(env, targetQN, 50, 0)\n",
    "print(np.mean(RL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reference\n",
    "\n",
    "In this iPython notebook I implement a Deep Q-Network using both Double DQN and Dueling DQN. The agent learn to solve a navigation task in a basic grid world. To learn more, read here: https://medium.com/p/8438a3e2b8df\n",
    "\n",
    "For more reinforcment learning tutorials, see:\n",
    "https://github.com/awjuliani/DeepRL-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
