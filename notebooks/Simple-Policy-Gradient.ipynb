{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "### Reference\n",
    "- Medium post by Arthur Juliani: [link](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.mtwpvfi8b)\n",
    "- Github for original code: [link](https://github.com/awjuliani/DeepRL-Agents)\n",
    "- Andrej Karpathy Blog post: [link](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os, sys, math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cart-Pole Problem\n",
    "- Task: Balance the pole on the cart without falling it down\n",
    "- State : observation in 4 tuple\n",
    "    1. x: position of the cart\n",
    "    2. theta: angle of the pole (default = 0, when vertical to the cart)\n",
    "    3. dx/dt : velocity of the cart\n",
    "    4. dtheta/dt : rate of change of the angle\n",
    "- Action: [Left, Right]\n",
    "- Reward: +1 for every time step (no negative reward -> need for reward standardization)\n",
    "- Goal: Reach for 200 score (stay balanced for 200 time frames)\n",
    "- 0.000001 change in state may result in huge Q-value difference\n",
    "- Need for flexible network to approximate the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary code for game environment\n",
    "1. env.reset(): reset the game environment to default position, restart episode\n",
    "2. env.render(): show visualiztion of current state\n",
    "3. next\\_state, reward, done, _ = env.step(action)\n",
    "    - next_state: next state after given action is taken \n",
    "    - reward: reward from the action, here all the reward is set to 1 \n",
    "    - done: if episode is ended. if done == True, break the episode loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 16 timesteps\n"
     ]
    }
   ],
   "source": [
    "# simulate environment with sample actions\n",
    "\n",
    "for i_episode in range(2):\n",
    "    state = env.reset()\n",
    "    for t in range(20):\n",
    "        env.render()\n",
    "        #print(state)\n",
    "        # sample random action from available action list\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        #print(reward)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graphing\n",
    "\n",
    "#### 1) Set up hyperparameters\n",
    "\n",
    "#### 2) Forward Propagation\n",
    "- Input: Current state as 1x4 tensor / Output: probability to take action \"right\"\n",
    "- Neural Network with 1 hidden layer: total 2 weights and 2 biass to be trained\n",
    "- Logit will be treated as \"score\" for a certain action\n",
    "\n",
    "#### 3) Back Propagation\n",
    "- Labels: actions taken (1 for right, 0 for left)\n",
    "- Logits: final output from neural network\n",
    "- Log-likelihood error: here, we use cross entropy method instead\n",
    "- Advantage: discounted cumulative future reward of given action\n",
    "- Loss: log-likelihood * Advantage\n",
    "- Advantage will guide the gradient toward or away from optimum\n",
    "- Optimizer: optimize with decaying learning rate, here we use Adam\n",
    "\n",
    "#### 4) Gradient Update\n",
    "- For stabilized learning, gradient will be updated slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## hyperparameters ##\n",
    "\n",
    "input_dim = 4 # input dimensionality\n",
    "hidden_dim = 10 # number of hidden layer neurons\n",
    "batch_size = 5 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-2 # feel free to play with this to train faster or more stably.\n",
    "gamma = 0.99 # discount factor for rewar\n",
    "total_episodes = 10000 # the number of episodes\n",
    "logdir = \"./ce_log\" # directory for storing summaries and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphing Done!\n"
     ]
    }
   ],
   "source": [
    "## Graph ##\n",
    "\n",
    "main_graph = tf.Graph()\n",
    "with main_graph.as_default():\n",
    "    \n",
    "    #This defines the network as it goes from taking an observation of the environment to \n",
    "    #giving a probability of chosing to the action of moving left or right\n",
    "    \n",
    "    with tf.name_scope(\"Variables\"):\n",
    "        \n",
    "        # for forward-propagation #\n",
    "        \n",
    "        # input_x will be current state containing 4 elements each\n",
    "        input_x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim] , name=\"input_x\")\n",
    "        \n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Neural network with 1 hidden layer\n",
    "        W1 = tf.Variable(name=\"input_hidden_W\", initial_value=xavier_init([input_dim, hidden_dim]), \n",
    "                         dtype=tf.float32)\n",
    "        b1 = tf.Variable(name=\"input_hidden_b\", initial_value=xavier_init([hidden_dim]), dtype=tf.float32)\n",
    "        \n",
    "        W2 = tf.Variable(name=\"hidden_output_W\", initial_value=xavier_init([hidden_dim, 1]), dtype=tf.float32)\n",
    "        b2 = tf.Variable(name=\"hidden_output_b\", initial_value=xavier_init([1]), dtype=tf.float32)\n",
    "        \n",
    "        # for back-propagation #\n",
    "        \n",
    "        # input_y will be actions from predicted policy\n",
    "        # advantages will be discounted reward sum of each state\n",
    "        # advantage will guide the network as a multiplier for applying gradients\n",
    "        input_y = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"input_y\")\n",
    "        advantages = tf.placeholder(dtype=tf.float32, name=\"reward_signal\")\n",
    "        \n",
    "        # Gradient Buffers #\n",
    "        \n",
    "        # Placeholders to send the final gradients through when we update.\n",
    "        W1Grad = tf.placeholder(dtype=tf.float32,name=\"batch_grad_W1\") \n",
    "        W2Grad = tf.placeholder(dtype=tf.float32,name=\"batch_grad_W2\")\n",
    "        b1Grad = tf.placeholder(dtype=tf.float32,name=\"batch_grad_b1\")\n",
    "        b2Grad = tf.placeholder(dtype=tf.float32,name=\"batch_grad_b2\")\n",
    "        \n",
    "        # Tensorboard summary\n",
    "        tf.summary.histogram(\"Input_to_hidden_Weight\", W1)\n",
    "        tf.summary.histogram(\"Input_to_hidden_bias\", b1)\n",
    "        tf.summary.histogram(\"Hidden_to_output_Weight\", W2)\n",
    "        tf.summary.histogram(\"Hidden_to_output_bias\", b2)\n",
    "        \n",
    "    with tf.name_scope(\"forward_propagation\"):\n",
    "        # Dimension Changes:\n",
    "        # input: [batch_size, 4] -> hidden: [batch_size, 10] -> output: [batch_size, 1]\n",
    "        \n",
    "        # first hidden layer utilizes ReLU activation\n",
    "        layer1 = tf.nn.relu(tf.add(tf.matmul(input_x, W1), b1))\n",
    "        \n",
    "        # we call final logit \"score\", it measures score for choosing certain action\n",
    "        # the whole network approximates scoring function.\n",
    "        score = tf.add(tf.matmul(layer1,W2), b2, name=\"score\")\n",
    "        \n",
    "        # sigmoid function as actiavtion as output type is binary (left or right)\n",
    "        # single number between 0 or 1 will be probability to go \"Right\"\n",
    "        probability = tf.nn.sigmoid(score)\n",
    "        \n",
    "        # Tensorboard summary\n",
    "        tf.summary.histogram(\"Score\", score)\n",
    "    \n",
    "    with tf.name_scope(\"back_propagation\"):\n",
    "        \n",
    "        # get the variables to be updated\n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        # The loss function. This sends the weights in the direction of making actions \n",
    "        # that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "        \n",
    "        # calculate loss between action taken (will be 1 or 0 like label) and score (logit from Neural Network) \n",
    "        loglik = tf.nn.sigmoid_cross_entropy_with_logits(labels=input_y, logits=score, name=\"cross_entropy\")\n",
    "        \n",
    "        # loss is multiplied with advantages to determine direction for optimization\n",
    "        # if advantage is positive, optimize by minimizing the loss\n",
    "        # elif advantage is negative, optimize by maximizing the loss\n",
    "        loss = tf.reduce_sum(loglik * advantages, name=\"Loss\") \n",
    "        \n",
    "        loglik2 = (input_y)*(-tf.log(input_y - probability)) + (1-input_y)*(-tf.log(input_y + probability))\n",
    "        #loss = -tf.reduce_mean(loglik * advantages) \n",
    "        \n",
    "        # calculate gradient of trainable variables\n",
    "        newGrads = tf.gradients(loss,tvars, name=\"Gradients\")\n",
    "        \n",
    "        tf.summary.scalar(\"Loss\", loss)\n",
    "        \n",
    "    with tf.name_scope(\"Gradient_update\"):\n",
    "        \n",
    "        # Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "        # We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "                \n",
    "        # merge gradients for update\n",
    "        batchGrad = [W1Grad,b1Grad,W2Grad,b2Grad]\n",
    "        \n",
    "        # Our optimizer\n",
    "        # Adam or RMSProp is preferred (Optimizer with decaying learning rate is recommended)\n",
    "        adam = tf.train.AdamOptimizer(learning_rate=learning_rate) \n",
    "        \n",
    "        # update gradient value\n",
    "        # apply gradients slowly\n",
    "        updateGrads = adam.apply_gradients(zip(batchGrad,tvars), name=\"update\")\n",
    "        \n",
    "    with tf.name_scope(\"utils\"):\n",
    "        \n",
    "        # make log directory\n",
    "        if not os.path.exists(logdir):\n",
    "            os.mkdir(logdir)\n",
    "        \n",
    "        # writer and saver for model\n",
    "        saver = tf.train.Saver()\n",
    "        writer = tf.summary.FileWriter(logdir=logdir)\n",
    "        summary = tf.summary.merge_all()\n",
    "        \n",
    "print(\"Graphing Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Calculating Advantage\n",
    "- get all the reward value\n",
    "- sum up rewards with discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    # initialize with zeros\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    # set start value\n",
    "    running_add = 0\n",
    "    \n",
    "    # add up gamma*r[t] for each step in episode\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "        \n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Train\n",
    "- load variables if checkpoint exists\n",
    "- stack state, action and reward into buffers\n",
    "- if episode is done, start the training based on stacked info\n",
    "- advantage will be discounted future reward calculated from reward buffer\n",
    "- Loss & Gradients computation\n",
    "- Update gradients slowly\n",
    "- Write summary and checkpoints\n",
    "- if average score exceeds 190, episodes will be visualized\n",
    "- if average score reaches 200, training will be finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters...\n",
      "INFO:tensorflow:Restoring parameters from ./ce_log/model.ckpt-250\n",
      "Average reward for episode 108.00.  Total average reward 108.00.\n",
      "Average reward for episode 121.00.  Total average reward 108.00.\n",
      "Average reward for episode 184.00.  Total average reward 109.00.\n",
      "Average reward for episode 157.00.  Total average reward 109.00.\n",
      "Average reward for episode 103.00.  Total average reward 109.00.\n",
      "ce:\n",
      "\n",
      "[[0.84547013]\n",
      " [0.3197832 ]\n",
      " [0.52871746]\n",
      " [0.55265975]\n",
      " [0.9205006 ]\n",
      " [0.3142205 ]\n",
      " [0.47200522]\n",
      " [0.76931536]\n",
      " [0.35048634]\n",
      " [0.6368471 ]\n",
      " [0.43424833]\n",
      " [0.66116554]\n",
      " [0.4147884 ]\n",
      " [0.6870347 ]\n",
      " [0.3959471 ]\n",
      " [0.7148518 ]\n",
      " [0.37898675]\n",
      " [0.74426216]\n",
      " [1.1929665 ]\n",
      " [0.29731312]\n",
      " [0.35012585]\n",
      " [0.845583  ]\n",
      " [0.34541193]\n",
      " [0.5190128 ]\n",
      " [0.55641127]\n",
      " [0.49308026]\n",
      " [0.5886071 ]\n",
      " [0.4719306 ]\n",
      " [0.77270555]\n",
      " [0.3475607 ]\n",
      " [0.7604523 ]\n",
      " [1.2186682 ]\n",
      " [0.17886172]\n",
      " [0.33420044]\n",
      " [0.804722  ]\n",
      " [0.3169293 ]\n",
      " [0.5596904 ]\n",
      " [0.52062374]\n",
      " [0.5336765 ]\n",
      " [0.86519724]\n",
      " [0.34094226]\n",
      " [0.5586247 ]\n",
      " [0.49531087]\n",
      " [0.5841728 ]\n",
      " [0.46996063]\n",
      " [0.6152386 ]\n",
      " [1.0301777 ]\n",
      " [0.22239618]\n",
      " [0.39610437]\n",
      " [0.66092557]\n",
      " [1.0380844 ]\n",
      " [1.3368852 ]\n",
      " [0.25859046]\n",
      " [1.3312068 ]\n",
      " [1.4758676 ]\n",
      " [0.2174718 ]\n",
      " [1.4785577 ]\n",
      " [1.6469499 ]\n",
      " [0.16840437]\n",
      " [0.2078018 ]\n",
      " [1.5107576 ]\n",
      " [0.19995874]\n",
      " [0.24088576]\n",
      " [0.27993324]\n",
      " [1.2853434 ]\n",
      " [0.26754075]\n",
      " [0.30866402]\n",
      " [0.35451382]]\n",
      "ll:\n",
      "\n",
      "[[        nan]\n",
      " [-0.54598415]\n",
      " [-0.46333164]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5483283 ]\n",
      " [-0.4847384 ]\n",
      " [-0.38071483]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.49941152]\n",
      " [        nan]\n",
      " [-0.5071067 ]\n",
      " [        nan]\n",
      " [-0.5146437 ]\n",
      " [        nan]\n",
      " [-0.5215012 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.55549955]\n",
      " [-0.5333301 ]\n",
      " [        nan]\n",
      " [-0.5352813 ]\n",
      " [-0.4669413 ]\n",
      " [        nan]\n",
      " [-0.47669506]\n",
      " [        nan]\n",
      " [-0.48476714]\n",
      " [-0.3796426 ]\n",
      " [        nan]\n",
      " [-0.38352957]\n",
      " [-0.25899208]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.3696391 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.46634057]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.53713644]\n",
      " [-0.45234567]\n",
      " [        nan]\n",
      " [-0.44312492]\n",
      " [        nan]\n",
      " [-0.4321148 ]\n",
      " [-0.30523476]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5721874 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5903114 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.6124858 ]\n",
      " [-0.5946343 ]\n",
      " [        nan]\n",
      " [-0.59815747]\n",
      " [-0.57994   ]\n",
      " [-0.56294405]\n",
      " [        nan]\n",
      " [-0.5682975 ]\n",
      " [-0.55067736]\n",
      " [-0.53151864]]\n",
      "Model saved!\n",
      "Summary written!\n",
      "Average reward for episode 87.00.  Total average reward 109.00.\n",
      "Average reward for episode 132.00.  Total average reward 109.00.\n",
      "Average reward for episode 127.00.  Total average reward 109.00.\n",
      "Average reward for episode 183.00.  Total average reward 110.00.\n",
      "Average reward for episode 123.00.  Total average reward 110.00.\n",
      "ce:\n",
      "\n",
      "[[0.5903132 ]\n",
      " [0.4648342 ]\n",
      " [0.76584584]\n",
      " [0.31848797]\n",
      " [0.7477918 ]\n",
      " [0.32712296]\n",
      " [0.65318847]\n",
      " [0.41791534]\n",
      " [0.68292046]\n",
      " [0.39439973]\n",
      " [0.72167146]\n",
      " [0.36708653]\n",
      " [0.6210951 ]\n",
      " [0.42202812]\n",
      " [0.5913665 ]\n",
      " [0.44890514]\n",
      " [0.8425418 ]\n",
      " [1.3549821 ]\n",
      " [0.14207603]\n",
      " [0.25616583]\n",
      " [1.0275754 ]\n",
      " [1.627543  ]\n",
      " [0.09833591]\n",
      " [0.17311099]\n",
      " [0.297832  ]\n",
      " [0.9556354 ]\n",
      " [0.24023543]\n",
      " [1.1274382 ]\n",
      " [0.18855017]\n",
      " [0.3116137 ]\n",
      " [0.87801105]\n",
      " [0.24917506]\n",
      " [0.42207998]\n",
      " [0.64489865]\n",
      " [1.2610887 ]\n",
      " [0.15199985]\n",
      " [1.5134733 ]\n",
      " [0.11207098]\n",
      " [0.18508992]\n",
      " [0.3325337 ]\n",
      " [0.8024022 ]\n",
      " [1.5134941 ]\n",
      " [2.339659  ]\n",
      " [0.04089166]\n",
      " [0.06666519]\n",
      " [0.11106455]]\n",
      "ll:\n",
      "\n",
      "[[-0.44093114]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5465293 ]\n",
      " [        nan]\n",
      " [-0.5429027 ]\n",
      " [-0.4189628 ]\n",
      " [        nan]\n",
      " [-0.40888563]\n",
      " [        nan]\n",
      " [-0.39604715]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.50423336]\n",
      " [        nan]\n",
      " [-0.49367538]\n",
      " [-0.35810414]\n",
      " [-0.22948498]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.30592   ]\n",
      " [-0.17932674]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.32538798]\n",
      " [        nan]\n",
      " [-0.2805531 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.3475596 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.42180794]\n",
      " [-0.2494702 ]\n",
      " [        nan]\n",
      " [-0.1989689 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.37035653]\n",
      " [-0.19896519]\n",
      " [-0.09199601]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]]\n",
      "Model saved!\n",
      "Summary written!\n",
      "Average reward for episode 126.00.  Total average reward 110.00.\n",
      "Average reward for episode 132.00.  Total average reward 110.00.\n",
      "Average reward for episode 142.00.  Total average reward 111.00.\n",
      "Average reward for episode 179.00.  Total average reward 111.00.\n",
      "Average reward for episode 169.00.  Total average reward 112.00.\n",
      "ce:\n",
      "\n",
      "[[0.7754208 ]\n",
      " [1.2458645 ]\n",
      " [0.16448629]\n",
      " [0.29584473]\n",
      " [0.50067043]\n",
      " [0.8669247 ]\n",
      " [0.29945403]\n",
      " [0.8237842 ]\n",
      " [0.30337772]\n",
      " [0.60091954]\n",
      " [0.43047565]\n",
      " [0.7507338 ]\n",
      " [0.32782227]\n",
      " [0.6564321 ]\n",
      " [0.3981968 ]\n",
      " [0.6907783 ]\n",
      " [0.37655583]\n",
      " [0.7312494 ]\n",
      " [0.35199007]\n",
      " [0.7794437 ]\n",
      " [1.2807051 ]\n",
      " [0.15739124]\n",
      " [0.2842328 ]\n",
      " [0.9489958 ]\n",
      " [0.2498082 ]\n",
      " [0.42661104]\n",
      " [0.77557385]\n",
      " [0.31253394]\n",
      " [0.7308284 ]\n",
      " [0.32996792]\n",
      " [0.6995926 ]\n",
      " [1.2341105 ]\n",
      " [0.21931666]\n",
      " [1.2507018 ]\n",
      " [0.2136057 ]\n",
      " [0.32081023]\n",
      " [0.637044  ]\n",
      " [0.3930576 ]\n",
      " [0.6274361 ]\n",
      " [0.39644977]\n",
      " [0.61989725]\n",
      " [0.398758  ]\n",
      " [0.6142306 ]\n",
      " [0.40003076]\n",
      " [0.78348005]\n",
      " [0.29707125]\n",
      " [0.58340204]\n",
      " [0.42292646]\n",
      " [0.57083255]\n",
      " [1.0503587 ]\n",
      " [0.22620304]\n",
      " [1.0725065 ]\n",
      " [0.21714033]\n",
      " [1.1113775 ]\n",
      " [0.20354137]\n",
      " [0.37216064]\n",
      " [0.65871644]\n",
      " [0.3546605 ]\n",
      " [0.69160986]\n",
      " [0.33601573]\n",
      " [0.72966117]\n",
      " [1.3073738 ]\n",
      " [0.15157065]\n",
      " [0.27916187]\n",
      " [0.54504836]\n",
      " [0.46142763]\n",
      " [0.91664386]\n",
      " [0.23439437]\n",
      " [0.46037054]\n",
      " [0.86081916]\n",
      " [0.28201002]\n",
      " [0.5682934 ]\n",
      " [0.41473573]\n",
      " [0.79740185]\n",
      " [1.3371054 ]\n",
      " [0.17621985]\n",
      " [0.29419538]\n",
      " [0.8319082 ]\n",
      " [0.28413078]\n",
      " [0.54409194]\n",
      " [1.0097446 ]\n",
      " [0.2165391 ]\n",
      " [1.0201505 ]\n",
      " [1.6637942 ]\n",
      " [0.09558815]\n",
      " [0.18984802]\n",
      " [0.38336322]\n",
      " [0.6382965 ]\n",
      " [1.194583  ]\n",
      " [0.16129276]\n",
      " [0.32180995]\n",
      " [0.74744046]\n",
      " [0.29169738]\n",
      " [0.80900025]\n",
      " [0.26033342]\n",
      " [0.5352187 ]\n",
      " [0.47137448]\n",
      " [0.9327688 ]\n",
      " [0.21288669]\n",
      " [1.0152657 ]\n",
      " [1.7639099 ]\n",
      " [0.08113458]\n",
      " [0.15452954]\n",
      " [1.2935377 ]\n",
      " [0.12711555]\n",
      " [0.25946686]\n",
      " [0.92337817]\n",
      " [0.21237881]\n",
      " [1.0652715 ]\n",
      " [0.16866337]\n",
      " [0.3449636 ]\n",
      " [0.64358425]\n",
      " [0.41289216]\n",
      " [0.83445895]\n",
      " [0.2504546 ]\n",
      " [0.48875478]\n",
      " [0.5317996 ]\n",
      " [0.43175787]\n",
      " [0.8013449 ]\n",
      " [1.2842929 ]\n",
      " [1.8858212 ]\n",
      " [0.09611175]\n",
      " [0.1572789 ]\n",
      " [0.30751255]\n",
      " [0.5692871 ]\n",
      " [0.43429095]\n",
      " [0.5588652 ]\n",
      " [0.44760922]\n",
      " [0.5463173 ]\n",
      " [0.9914926 ]\n",
      " [1.6731231 ]\n",
      " [2.511521  ]\n",
      " [0.03658476]\n",
      " [0.07323062]\n",
      " [1.9009535 ]\n",
      " [0.06236926]\n",
      " [0.13316113]\n",
      " [0.29627606]\n",
      " [0.8236085 ]\n",
      " [0.25848752]\n",
      " [0.9167482 ]\n",
      " [0.220603  ]\n",
      " [0.4452267 ]\n",
      " [0.59470654]\n",
      " [0.39890644]\n",
      " [0.73891664]\n",
      " [0.3619825 ]\n",
      " [0.68331057]\n",
      " [1.2424551 ]\n",
      " [0.13352735]\n",
      " [1.354178  ]\n",
      " [2.2432275 ]\n",
      " [0.03967815]\n",
      " [0.08609898]\n",
      " [1.7375277 ]\n",
      " [0.06519119]\n",
      " [0.14499153]\n",
      " [0.2962389 ]\n",
      " [0.55618155]\n",
      " [0.9065097 ]\n",
      " [0.28780505]\n",
      " [0.83623666]\n",
      " [0.3138367 ]\n",
      " [0.78876764]\n",
      " [1.2631719 ]\n",
      " [0.16669591]\n",
      " [1.2689998 ]\n",
      " [0.16121362]\n",
      " [0.3131341 ]\n",
      " [0.5756473 ]\n",
      " [0.4698046 ]\n",
      " [0.55845296]\n",
      " [0.95376515]\n",
      " [1.587959  ]\n",
      " [0.08810797]]\n",
      "ll:\n",
      "\n",
      "[[-0.3787857 ]\n",
      " [-0.25285155]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.55458766]\n",
      " [        nan]\n",
      " [-0.5529192 ]\n",
      " [-0.4371622 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.54260975]\n",
      " [-0.41785386]\n",
      " [        nan]\n",
      " [-0.4062554 ]\n",
      " [        nan]\n",
      " [-0.39292496]\n",
      " [        nan]\n",
      " [-0.37751892]\n",
      " [-0.2451722 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.32723653]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5490405 ]\n",
      " [        nan]\n",
      " [-0.5417118 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.58948934]\n",
      " [        nan]\n",
      " [-0.59203696]\n",
      " [-0.54555213]\n",
      " [-0.42451805]\n",
      " [        nan]\n",
      " [-0.42785206]\n",
      " [        nan]\n",
      " [-0.43048272]\n",
      " [        nan]\n",
      " [-0.43246862]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.55560255]\n",
      " [-0.44340086]\n",
      " [        nan]\n",
      " [-0.4479208 ]\n",
      " [-0.29996553]\n",
      " [        nan]\n",
      " [-0.29427272]\n",
      " [        nan]\n",
      " [-0.28450608]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.41707423]\n",
      " [        nan]\n",
      " [-0.40597782]\n",
      " [        nan]\n",
      " [-0.39344135]\n",
      " [-0.23943384]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.48881495]\n",
      " [-0.33637133]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.56205064]\n",
      " [-0.44883832]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.6089139 ]\n",
      " [-0.55682963]\n",
      " [        nan]\n",
      " [-0.56113935]\n",
      " [-0.45765805]\n",
      " [-0.3106502 ]\n",
      " [        nan]\n",
      " [-0.30788213]\n",
      " [-0.17346488]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.42408496]\n",
      " [-0.26453897]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.38769287]\n",
      " [        nan]\n",
      " [-0.36831903]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.48498082]\n",
      " [-0.33179188]\n",
      " [        nan]\n",
      " [-0.30917892]\n",
      " [-0.15817697]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.24239595]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.3344524 ]\n",
      " [        nan]\n",
      " [-0.29612207]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.50786144]\n",
      " [-0.3605439 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.4621898 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.64624554]\n",
      " [-0.6175966 ]\n",
      " [-0.5511651 ]\n",
      " [-0.44847906]\n",
      " [        nan]\n",
      " [-0.45225817]\n",
      " [        nan]\n",
      " [-0.4568414 ]\n",
      " [-0.3155568 ]\n",
      " [-0.17198507]\n",
      " [-0.07802038]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.13926277]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.36384082]\n",
      " [        nan]\n",
      " [-0.3363415 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.43936685]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5284462 ]\n",
      " [-0.4087548 ]\n",
      " [-0.25361425]\n",
      " [        nan]\n",
      " [-0.22964986]\n",
      " [-0.10085429]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.16208044]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.55956304]\n",
      " [        nan]\n",
      " [-0.5484903 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.6132687 ]\n",
      " [        nan]\n",
      " [-0.6157856 ]\n",
      " [-0.548787  ]\n",
      " [-0.44618517]\n",
      " [        nan]\n",
      " [-0.45240813]\n",
      " [-0.3259078 ]\n",
      " [-0.18593356]\n",
      " [        nan]]\n",
      "Model saved!\n",
      "Summary written!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 123.00.  Total average reward 112.00.\n",
      "Average reward for episode 143.00.  Total average reward 112.00.\n",
      "Average reward for episode 177.00.  Total average reward 113.00.\n",
      "Average reward for episode 184.00.  Total average reward 114.00.\n",
      "Average reward for episode 134.00.  Total average reward 114.00.\n",
      "ce:\n",
      "\n",
      "[[0.71016055]\n",
      " [0.32665202]\n",
      " [0.64166284]\n",
      " [0.42606977]\n",
      " [0.62786865]\n",
      " [0.4331893 ]\n",
      " [0.6193984 ]\n",
      " [0.4369817 ]\n",
      " [0.6158227 ]\n",
      " [1.0374438 ]\n",
      " [0.21865831]\n",
      " [0.41570735]\n",
      " [0.6572789 ]\n",
      " [0.40118203]\n",
      " [0.6830585 ]\n",
      " [0.38348252]\n",
      " [0.7161209 ]\n",
      " [0.36262396]\n",
      " [0.7578927 ]\n",
      " [0.33867857]\n",
      " [0.59153056]\n",
      " [1.0676621 ]\n",
      " [0.26547658]\n",
      " [0.41988176]\n",
      " [0.81663764]\n",
      " [0.31006953]\n",
      " [0.5556882 ]\n",
      " [1.0101191 ]\n",
      " [0.26982105]\n",
      " [0.44328988]\n",
      " [0.84196824]\n",
      " [0.2988909 ]\n",
      " [0.869159  ]\n",
      " [0.28422835]\n",
      " [0.9084532 ]\n",
      " [0.26521578]\n",
      " [0.48168173]\n",
      " [0.5373865 ]\n",
      " [0.4609708 ]\n",
      " [0.5642125 ]\n",
      " [0.4408855 ]\n",
      " [0.8053514 ]\n",
      " [0.299138  ]\n",
      " [0.7996396 ]\n",
      " [0.29703617]\n",
      " [0.8076976 ]\n",
      " [1.3713989 ]\n",
      " [0.20931664]\n",
      " [0.28009763]\n",
      " [0.90840065]\n",
      " [0.2691462 ]\n",
      " [0.9826714 ]\n",
      " [0.25617003]\n",
      " [0.4145533 ]\n",
      " [0.75940865]\n",
      " [0.35423392]\n",
      " [0.72642016]\n",
      " [0.37278917]\n",
      " [0.6847663 ]\n",
      " [1.2479157 ]\n",
      " [0.23037377]\n",
      " [0.3002059 ]\n",
      " [0.56291026]\n",
      " [0.49182704]\n",
      " [0.51796407]\n",
      " [0.529252  ]\n",
      " [0.47813067]\n",
      " [0.8374329 ]\n",
      " [0.30350178]\n",
      " [0.583161  ]\n",
      " [1.0480278 ]\n",
      " [1.4511118 ]\n",
      " [0.20764796]\n",
      " [0.25956556]\n",
      " [0.3371976 ]\n",
      " [0.79346347]\n",
      " [0.3090394 ]\n",
      " [0.87561244]\n",
      " [0.30073985]\n",
      " [0.9655644 ]\n",
      " [1.3672326 ]\n",
      " [1.5635011 ]\n",
      " [0.17992759]\n",
      " [0.22477184]\n",
      " [0.27515513]\n",
      " [0.325396  ]\n",
      " [0.43291435]\n",
      " [0.70644796]\n",
      " [0.38090765]\n",
      " [0.58424115]\n",
      " [0.8978167 ]\n",
      " [0.30352423]\n",
      " [0.8265775 ]\n",
      " [0.3352342 ]\n",
      " [0.6154566 ]\n",
      " [0.443408  ]\n",
      " [0.65986556]\n",
      " [0.41294312]\n",
      " [0.6845942 ]\n",
      " [0.41764754]\n",
      " [0.66478914]\n",
      " [0.42673704]\n",
      " [0.72955114]\n",
      " [0.3851867 ]\n",
      " [0.64572567]\n",
      " [0.43027863]\n",
      " [0.6540061 ]\n",
      " [1.0836947 ]\n",
      " [0.19782625]\n",
      " [1.1747849 ]\n",
      " [0.17030436]\n",
      " [0.3130434 ]\n",
      " [0.8614522 ]\n",
      " [1.4696686 ]\n",
      " [0.11149966]\n",
      " [0.19898576]\n",
      " [1.2324822 ]\n",
      " [0.14731456]\n",
      " [0.25283316]\n",
      " [1.0745215 ]\n",
      " [0.18028457]\n",
      " [0.29736367]\n",
      " [0.9819482 ]\n",
      " [0.20856227]\n",
      " [1.2494446 ]\n",
      " [0.13935463]\n",
      " [1.6003183 ]\n",
      " [0.08791708]]\n",
      "ll:\n",
      "\n",
      "[[        nan]\n",
      " [-0.5431    ]\n",
      " [-0.4229227 ]\n",
      " [        nan]\n",
      " [-0.42770147]\n",
      " [        nan]\n",
      " [-0.43065718]\n",
      " [        nan]\n",
      " [-0.4319099 ]\n",
      " [-0.30332857]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.41756472]\n",
      " [        nan]\n",
      " [-0.40883932]\n",
      " [        nan]\n",
      " [-0.3978657 ]\n",
      " [        nan]\n",
      " [-0.38434565]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5691929 ]\n",
      " [-0.5050839 ]\n",
      " [-0.3659721 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5673096 ]\n",
      " [-0.49586692]\n",
      " [-0.35827675]\n",
      " [        nan]\n",
      " [-0.35016662]\n",
      " [        nan]\n",
      " [-0.3387178 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.46012583]\n",
      " [        nan]\n",
      " [-0.450316  ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.55472213]\n",
      " [        nan]\n",
      " [-0.55561763]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5939556 ]\n",
      " [-0.56287324]\n",
      " [        nan]\n",
      " [-0.5676018 ]\n",
      " [        nan]\n",
      " [-0.5732428 ]\n",
      " [-0.5072002 ]\n",
      " [-0.38386214]\n",
      " [        nan]\n",
      " [-0.39449662]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.58457965]\n",
      " [-0.55426764]\n",
      " [-0.45078838]\n",
      " [        nan]\n",
      " [-0.4673327 ]\n",
      " [        nan]\n",
      " [-0.48238987]\n",
      " [-0.35964468]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5947033 ]\n",
      " [-0.5717626 ]\n",
      " [-0.5386943 ]\n",
      " [        nan]\n",
      " [-0.55051845]\n",
      " [        nan]\n",
      " [-0.5540405 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.6072247 ]\n",
      " [-0.5870633 ]\n",
      " [-0.5650037 ]\n",
      " [-0.54362655]\n",
      " [-0.4999362 ]\n",
      " [        nan]\n",
      " [-0.5207211 ]\n",
      " [-0.44310042]\n",
      " [-0.34178543]\n",
      " [        nan]\n",
      " [-0.36293626]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.49582076]\n",
      " [        nan]\n",
      " [-0.5078411 ]\n",
      " [-0.40832427]\n",
      " [        nan]\n",
      " [-0.41500735]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5189864 ]\n",
      " [-0.42152345]\n",
      " [        nan]\n",
      " [-0.41868302]\n",
      " [-0.29143238]\n",
      " [        nan]\n",
      " [-0.26917598]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.35244966]\n",
      " [-0.20701553]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.25585693]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.29375938]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.31814867]\n",
      " [        nan]\n",
      " [-0.2520528 ]\n",
      " [        nan]\n",
      " [-0.1838473 ]\n",
      " [        nan]]\n",
      "Model saved!\n",
      "Summary written!\n",
      "Average reward for episode 152.00.  Total average reward 114.00.\n",
      "Average reward for episode 167.00.  Total average reward 115.00.\n",
      "Average reward for episode 172.00.  Total average reward 115.00.\n",
      "Average reward for episode 166.00.  Total average reward 116.00.\n",
      "Average reward for episode 180.00.  Total average reward 117.00.\n",
      "ce:\n",
      "\n",
      "[[0.5660616 ]\n",
      " [0.41347867]\n",
      " [0.5335749 ]\n",
      " [1.0362867 ]\n",
      " [0.2479475 ]\n",
      " [0.43898982]\n",
      " [0.9063176 ]\n",
      " [0.24702175]\n",
      " [0.48688093]\n",
      " [0.48332953]\n",
      " [0.9772586 ]\n",
      " [0.21963747]\n",
      " [0.43732652]\n",
      " [0.5473591 ]\n",
      " [1.076594  ]\n",
      " [0.18737338]\n",
      " [0.3761549 ]\n",
      " [0.7460436 ]\n",
      " [0.30366233]\n",
      " [0.7242708 ]\n",
      " [0.30977604]\n",
      " [0.71535176]\n",
      " [0.3104879 ]\n",
      " [0.71856123]\n",
      " [0.3058585 ]\n",
      " [0.7339121 ]\n",
      " [0.29605338]\n",
      " [0.62860703]\n",
      " [0.37524685]\n",
      " [0.7715586 ]\n",
      " [0.2781269 ]\n",
      " [0.58680993]\n",
      " [0.40618038]\n",
      " [0.8346812 ]\n",
      " [0.2582607 ]\n",
      " [0.89285445]\n",
      " [0.24745038]\n",
      " [0.97474957]\n",
      " [0.23464654]\n",
      " [0.4134252 ]\n",
      " [0.5812355 ]\n",
      " [0.36998707]\n",
      " [0.6428616 ]\n",
      " [1.2639041 ]\n",
      " [0.21442173]\n",
      " [1.403048  ]\n",
      " [0.20602357]\n",
      " [0.26044685]\n",
      " [1.035775  ]\n",
      " [0.25125578]\n",
      " [1.1748397 ]\n",
      " [0.24145147]\n",
      " [0.3128071 ]\n",
      " [0.53981835]\n",
      " [0.90693885]\n",
      " [0.27194685]\n",
      " [0.5869065 ]\n",
      " [0.4232493 ]\n",
      " [0.67426753]\n",
      " [0.37664655]\n",
      " [0.6367447 ]\n",
      " [0.42511013]\n",
      " [0.817127  ]\n",
      " [0.32176688]\n",
      " [0.52138704]\n",
      " [0.5453138 ]\n",
      " [0.47628486]\n",
      " [0.59311193]\n",
      " [0.44035888]\n",
      " [0.74818516]\n",
      " [0.34366956]\n",
      " [0.6696236 ]\n",
      " [0.40221772]\n",
      " [0.70790267]\n",
      " [1.1441602 ]\n",
      " [0.2878255 ]\n",
      " [0.36163956]\n",
      " [0.8186644 ]\n",
      " [0.35511664]\n",
      " [0.8782969 ]\n",
      " [1.223542  ]\n",
      " [0.2716118 ]\n",
      " [0.33923388]\n",
      " [0.42824796]\n",
      " [0.65625644]\n",
      " [0.3944262 ]\n",
      " [0.6754354 ]\n",
      " [0.40713224]\n",
      " [0.7454448 ]\n",
      " [0.37513202]\n",
      " [0.6047796 ]\n",
      " [0.45472127]\n",
      " [0.5859925 ]\n",
      " [0.46610317]\n",
      " [0.5767544 ]\n",
      " [0.98083186]\n",
      " [0.2226349 ]\n",
      " [0.4485941 ]\n",
      " [0.6087368 ]\n",
      " [0.42107177]\n",
      " [0.63832486]\n",
      " [1.1393291 ]\n",
      " [0.16632491]\n",
      " [1.2755651 ]\n",
      " [0.1351318 ]\n",
      " [0.26214212]\n",
      " [0.4853561 ]\n",
      " [0.8326982 ]\n",
      " [0.36939836]\n",
      " [0.65150166]\n",
      " [1.2365057 ]\n",
      " [0.13867159]\n",
      " [0.26128381]\n",
      " [0.46948704]\n",
      " [0.62309325]\n",
      " [1.1736393 ]\n",
      " [0.14785504]\n",
      " [0.26881236]\n",
      " [0.4661411 ]\n",
      " [0.74229586]\n",
      " [0.37245005]\n",
      " [0.77720654]\n",
      " [0.28685364]\n",
      " [0.48715082]\n",
      " [0.83786803]\n",
      " [0.3520113 ]\n",
      " [0.7013936 ]\n",
      " [1.1331277 ]\n",
      " [0.27831373]\n",
      " [1.0811448 ]\n",
      " [0.28963083]\n",
      " [1.0077362 ]\n",
      " [0.29848334]\n",
      " [0.48458457]\n",
      " [0.9666195 ]\n",
      " [0.22799493]\n",
      " [0.42689058]\n",
      " [0.7941413 ]\n",
      " [1.2318851 ]\n",
      " [0.24790113]\n",
      " [0.34175956]\n",
      " [0.6166512 ]\n",
      " [0.39729026]\n",
      " [0.7535384 ]\n",
      " [0.3451347 ]\n",
      " [0.7622689 ]\n",
      " [1.2457092 ]\n",
      " [1.5463617 ]\n",
      " [0.1672742 ]\n",
      " [0.22064547]\n",
      " [0.28855175]\n",
      " [0.4274609 ]\n",
      " [0.79443026]\n",
      " [0.3313254 ]\n",
      " [0.7438355 ]\n",
      " [0.3590507 ]\n",
      " [0.70225775]\n",
      " [0.384403  ]\n",
      " [0.72498643]\n",
      " [1.2564924 ]\n",
      " [0.23584978]\n",
      " [0.30572748]\n",
      " [0.5016278 ]\n",
      " [0.85523045]\n",
      " [0.2828519 ]\n",
      " [0.58647704]\n",
      " [1.073672  ]\n",
      " [1.4473472 ]\n",
      " [0.2140853 ]\n",
      " [0.2561386 ]\n",
      " [0.3241664 ]\n",
      " [0.5305269 ]\n",
      " [0.9026261 ]\n",
      " [0.26629668]\n",
      " [0.56517404]\n",
      " [1.031791  ]\n",
      " [0.28706408]\n",
      " [1.1484575 ]\n",
      " [0.28041786]\n",
      " [0.35022685]\n",
      " [0.5616058 ]\n",
      " [0.97946966]\n",
      " [0.2346115 ]\n",
      " [0.9345486 ]\n",
      " [1.5254996 ]\n",
      " [0.10912056]\n",
      " [0.23550688]\n",
      " [0.48040602]\n",
      " [0.8779142 ]\n",
      " [0.32870725]\n",
      " [0.8922699 ]\n",
      " [1.2757608 ]\n",
      " [0.26153344]\n",
      " [0.3211677 ]\n",
      " [0.46714348]\n",
      " [0.847658  ]\n",
      " [0.27217972]\n",
      " [0.86150926]\n",
      " [0.26200104]\n",
      " [0.52325714]]\n",
      "ll:\n",
      "\n",
      "[[        nan]\n",
      " [-0.50762796]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.57683855]\n",
      " [-0.49755034]\n",
      " [-0.33933187]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.4804035 ]\n",
      " [-0.31942877]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.4564595 ]\n",
      " [-0.29323223]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.55279833]\n",
      " [        nan]\n",
      " [-0.55020666]\n",
      " [        nan]\n",
      " [-0.5499055 ]\n",
      " [        nan]\n",
      " [-0.5518663 ]\n",
      " [        nan]\n",
      " [-0.55603665]\n",
      " [-0.4274446 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.563722  ]\n",
      " [-0.4421817 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5723311 ]\n",
      " [        nan]\n",
      " [-0.5770565 ]\n",
      " [        nan]\n",
      " [-0.5826906 ]\n",
      " [-0.5076492 ]\n",
      " [        nan]\n",
      " [-0.5251682 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5916725 ]\n",
      " [        nan]\n",
      " [-0.5954318 ]\n",
      " [-0.57137895]\n",
      " [        nan]\n",
      " [-0.57538986]\n",
      " [        nan]\n",
      " [-0.5796912 ]\n",
      " [-0.5489251 ]\n",
      " [-0.45922965]\n",
      " [-0.33915314]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.50374985]\n",
      " [        nan]\n",
      " [-0.5224529 ]\n",
      " [-0.4246216 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.54515   ]\n",
      " [-0.46605614]\n",
      " [        nan]\n",
      " [-0.4830966 ]\n",
      " [        nan]\n",
      " [-0.49701384]\n",
      " [-0.38745362]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.51212585]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.55955434]\n",
      " [-0.52858704]\n",
      " [        nan]\n",
      " [-0.5312702 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5665347 ]\n",
      " [-0.5378467 ]\n",
      " [-0.50177467]\n",
      " [        nan]\n",
      " [-0.51525587]\n",
      " [-0.41140392]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.52306956]\n",
      " [-0.43579692]\n",
      " [        nan]\n",
      " [-0.44247383]\n",
      " [        nan]\n",
      " [-0.44578677]\n",
      " [-0.31845304]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.4344008 ]\n",
      " [        nan]\n",
      " [-0.42407516]\n",
      " [-0.27765727]\n",
      " [        nan]\n",
      " [-0.24629198]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.52540874]\n",
      " [-0.41954052]\n",
      " [-0.25495005]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.42936587]\n",
      " [-0.26944637]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5241627 ]\n",
      " [-0.37822294]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.53255117]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.56364155]\n",
      " [        nan]\n",
      " [-0.5587811 ]\n",
      " [        nan]\n",
      " [-0.55500096]\n",
      " [-0.47992486]\n",
      " [-0.32234916]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5768589 ]\n",
      " [-0.53679687]\n",
      " [-0.43161935]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5353962 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.61300355]\n",
      " [-0.5888977 ]\n",
      " [-0.5592432 ]\n",
      " [-0.5020852 ]\n",
      " [-0.3728307 ]\n",
      " [        nan]\n",
      " [-0.38885283]\n",
      " [        nan]\n",
      " [-0.40243745]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5821594 ]\n",
      " [-0.55192184]\n",
      " [-0.4734627 ]\n",
      " [-0.35430175]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.59182274]\n",
      " [-0.5732565 ]\n",
      " [-0.54414237]\n",
      " [-0.4626611 ]\n",
      " [-0.3403955 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5598807 ]\n",
      " [        nan]\n",
      " [-0.5627355 ]\n",
      " [-0.5332884 ]\n",
      " [-0.45126188]\n",
      " [-0.31882474]\n",
      " [        nan]\n",
      " [-0.33128962]\n",
      " [-0.19680974]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5422392 ]\n",
      " [        nan]\n",
      " [        nan]\n",
      " [-0.5709061 ]\n",
      " [-0.5454019 ]\n",
      " [-0.48660877]\n",
      " [-0.356567  ]\n",
      " [        nan]\n",
      " [-0.35243267]\n",
      " [        nan]\n",
      " [        nan]]\n",
      "Model saved!\n",
      "Summary written!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 171.00.  Total average reward 117.00.\n",
      "Average reward for episode 160.00.  Total average reward 118.00.\n",
      "Average reward for episode 119.00.  Total average reward 118.00.\n",
      "Average reward for episode 184.00.  Total average reward 118.00.\n",
      "Average reward for episode 177.00.  Total average reward 119.00.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-78a1c20da7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode_number\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisode_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 curr_summary = sess.run(summary, feed_dict={input_x: episode_states, input_y: episode_actions, \n\u001b[1;32m    135\u001b[0m                                                  advantages: discounted_reward_ep})\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0mexport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m         clear_extraneous_savers=clear_extraneous_savers)\n\u001b[0m\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, **kwargs)\u001b[0m\n\u001b[1;32m   1915\u001b[0m       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1918\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     69\u001b[0m                                         text_format.MessageToString(graph_def))\n\u001b[1;32m     70\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mSerializeToString\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[1;32m   1041\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[0;32m-> 1042\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes, deterministic)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value, deterministic)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m       \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/encoder.py\u001b[0m in \u001b[0;36mRepeatedFieldSize\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlocal_VarintSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1017\u001b[0m       \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields_by_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36mListFields\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m     \u001b[0mall_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsPresent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m     \u001b[0mall_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m     \u001b[0mall_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_IsPresent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m     \u001b[0mall_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/python_message.py\u001b[0m in \u001b[0;36m_IsPresent\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_FieldDescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLABEL_REPEATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_FieldDescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/google/protobuf/internal/containers.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set necessary arrays and values\n",
    "state_buffer, reward_buffer, action_buffer = [],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session(graph=main_graph) as sess:\n",
    "\n",
    "    # check for TensorFlow checkpoint\n",
    "    # if exists, restore Variables\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    \n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        print(\"Reading model parameters...\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    else:\n",
    "        print(\"Initializing all variables...\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # add graph to summary\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    # we will not see the graphics during training\n",
    "    rendering = False\n",
    "    \n",
    "    # Obtain an initial observation of the environment\n",
    "    state = env.reset() \n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "\n",
    "    # initialize Gradient buffer by filling with 0\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "        \n",
    "        if reward_sum / batch_size > 190 or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        state_input = np.reshape(state, [1, input_dim])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        curr_policy = sess.run(probability, feed_dict={input_x:state_input})\n",
    "        \n",
    "        # get the action from predicted policy\n",
    "        action = 1 if np.random.uniform() < curr_policy else 0\n",
    "        #y = 0 if action==1 else 1\n",
    "        y = action\n",
    "        \n",
    "        # add state info\n",
    "        state_buffer.append(state_input)\n",
    "        \n",
    "        # add action info\n",
    "        action_buffer.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # move to next state\n",
    "        state = next_state\n",
    "        \n",
    "        # add up reward\n",
    "        reward_sum += reward\n",
    "        \n",
    "        # record reward \n",
    "        reward_buffer.append(reward) \n",
    "        \n",
    "        # if episode is finished\n",
    "        if done:\n",
    "            # add episode\n",
    "            episode_number += 1\n",
    "            \n",
    "            # stack together all inputs, actions and reward for each episode\n",
    "            episode_states = np.vstack(state_buffer)\n",
    "            episode_actions = np.vstack(action_buffer)\n",
    "            episode_rewards = np.vstack(reward_buffer)\n",
    "            \n",
    "            # reset array memory\n",
    "            state_buffer, reward_buffer, action_buffer = [],[],[] \n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_reward_ep = discount_rewards(episode_rewards)\n",
    "            \n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            # first half would be Good actions (pos) while latter hald would be Bad actions (neg)\n",
    "            discounted_reward_ep -= np.mean(discounted_reward_ep)\n",
    "            discounted_reward_ep //= np.std(discounted_reward_ep)\n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            # x: states / y: actions / Advantage: discounted reward\n",
    "            tGrad = sess.run(newGrads,feed_dict={input_x: episode_states, input_y: episode_actions, \n",
    "                                                 advantages: discounted_reward_ep})\n",
    "            \n",
    "            # save into buffer, do not update right away\n",
    "            # prevent unstable learning\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            # assgin values from buffer\n",
    "            if episode_number % batch_size == 0: \n",
    "                grad_buffer_feeddict = {W1Grad: gradBuffer[0],b1Grad:gradBuffer[1], \n",
    "                                        W2Grad: gradBuffer[2],b2Grad: gradBuffer[3]}\n",
    "                \n",
    "                sess.run(updateGrads,feed_dict=grad_buffer_feeddict)\n",
    "                \n",
    "                # reset grad buffer to 0\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                ## Result printing\n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Average reward for episode {:.2f}.  Total average reward {:.2f}.' \n",
    "                      .format(reward_sum//batch_size, running_reward//batch_size))\n",
    "                \n",
    "                if reward_sum//batch_size >= 200: \n",
    "                    print(\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "                \n",
    "            if episode_number % (5*batch_size) == 0: \n",
    "                saver.save(sess, os.path.join(logdir, \"model.ckpt\"), global_step=episode_number)\n",
    "                curr_summary = sess.run(summary, feed_dict={input_x: episode_states, input_y: episode_actions, \n",
    "                                                 advantages: discounted_reward_ep})\n",
    "                curr_loss, curr_loss_1 = sess.run([loglik, loglik2], feed_dict={input_x: episode_states, input_y: episode_actions, \n",
    "                                                 advantages: discounted_reward_ep})\n",
    "                print(\"ce:\\n\")\n",
    "                print(curr_loss)\n",
    "                print(\"ll:\\n\")\n",
    "                print(curr_loss_1)\n",
    "                writer.add_summary(curr_summary, global_step=episode_number)\n",
    "                print(\"Model saved!\")\n",
    "                print(\"Summary written!\")\n",
    "            \n",
    "            state = env.reset()\n",
    "        \n",
    "print(episode_number,'Episodes completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python TensorFlow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
